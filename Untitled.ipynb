{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "version = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot(y_train):\n",
    "    import numpy as np\n",
    "\n",
    "    onehot = np.zeros([len(y_train),10])\n",
    "    for (i,val) in enumerate(y_train) : \n",
    "        onehot[i,val] = 1\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import utils.mnist_reader as mnist_reader\n",
    "X_train, y_train = mnist_reader.load_mnist('data/fashion', kind='train')\n",
    "X_test, y_test = mnist_reader.load_mnist('data/fashion', kind='t10k')\n",
    "\n",
    "y_train = one_hot(y_train)\n",
    "y_test = one_hot(y_test)\n",
    "\n",
    "if version == 1:\n",
    "    X_train = np.reshape(X_train, [-1,28,28,1])\n",
    "    X_test = np.reshape(X_test, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% version = 1 or 2\n",
    "# hyper-param\n",
    "ntrain, input_h, input_w, input_c = X_train.shape\n",
    "_, n_classes  = y_train.shape \n",
    "\n",
    "# INPUTS AND OUTPUTS\n",
    "x = tf.placeholder(\"float\", [None, input_h,input_w,input_c])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def weight_variable(name,shape):\n",
    "    with tf.variable_scope(name):\n",
    "        return tf.get_variable('W',shape=shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "       \n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def wb_variable(name,w_shape,b_shape):\n",
    "    with tf.variable_scope(name):\n",
    "        weight = tf.get_variable('W',shape=w_shape, initializer=tf.contrib.layers.xavier_initializer())\n",
    "        bias = tf.get_variable('b', shape=b_shape, initializer=tf.random_uniform_initializer(-1,1))\n",
    "        return weight, bias\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.relu(tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME'))\n",
    "\n",
    "#def conv2d(x, W, b):\n",
    "#  return tf.nn.relu(tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME') + b)\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% version = 5\n",
    "# variable\n",
    "k = 8 # growth rate\n",
    "W_conv1,b_conv1 = wb_variable('conv1',[3,3,1,k],[k])\n",
    "\n",
    "W_bnck1_1,b_bnck1_1 = wb_variable('bnck1_1',[1,1,k,k],[k])\n",
    "W_conv1_1,b_conv1_1 = wb_variable('conv1_1',[3,3,k,k],[k])\n",
    "W_bnck1_2,b_bnck1_2 = wb_variable('bnck1_2',[1,1,2*k,k],[k])\n",
    "W_conv1_2,b_conv1_2 = wb_variable('conv1_2',[3,3,k,k],[k])\n",
    "W_bnck1_3,b_bnck1_3 = wb_variable('bnck1_3',[1,1,3*k,k],[k])\n",
    "W_conv1_3,b_conv1_3 = wb_variable('conv1_3',[3,3,k,k],[k])\n",
    "W_bnck1_4,b_bnck1_4 = wb_variable('bnck1_4',[1,1,4*k,k],[k])\n",
    "W_conv1_4,b_conv1_4 = wb_variable('conv1_4',[3,3,k,k],[k])\n",
    "W_bnck1_5,b_bnck1_5 = wb_variable('bnck1_5',[1,1,5*k,k],[k])\n",
    "W_conv1_5,b_conv1_5 = wb_variable('conv1_5',[3,3,k,k],[k])\n",
    "\n",
    "W_bnck2_1,b_bnck2_1 = wb_variable('bnck2_1',[1,1,k,k],[k])\n",
    "W_conv2_1,b_conv2_1 = wb_variable('conv2_1',[3,3,k,k],[k])\n",
    "W_bnck2_2,b_bnck2_2 = wb_variable('bnck2_2',[1,1,2*k,k],[k])\n",
    "W_conv2_2,b_conv2_2 = wb_variable('conv2_2',[3,3,k,k],[k])\n",
    "W_bnck2_3,b_bnck2_3 = wb_variable('bnck2_3',[1,1,3*k,k],[k])\n",
    "W_conv2_3,b_conv2_3 = wb_variable('conv2_3',[3,3,k,k],[k])\n",
    "W_bnck2_4,b_bnck2_4 = wb_variable('bnck2_4',[1,1,4*k,k],[k])\n",
    "W_conv2_4,b_conv2_4 = wb_variable('conv2_4',[3,3,k,k],[k])\n",
    "W_bnck2_5,b_bnck2_5 = wb_variable('bnck2_5',[1,1,5*k,k],[k])\n",
    "W_conv2_5,b_conv2_5 = wb_variable('conv2_5',[3,3,k,k],[k])\n",
    "\n",
    "W_fc1 = weight_variable('fc1',[14 * 14 * k, 512])\n",
    "b_fc1 = bias_variable([512])\n",
    "W_fc2 = weight_variable('fc2',[512, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "# model\n",
    "h_conv1 = conv2d(x,W_conv1,b_conv1)\n",
    "h_bnck1_1 = conv2d(h_conv1,W_bnck1_1,b_bnck1_1)\n",
    "h_conv1_1 = conv2d(h_bnck1_1,W_conv1_1,b_conv1_1)\n",
    "h_bnck1_2 = conv2d(tf.concat((h_conv1,h_conv1_1),axis=3),W_bnck1_2,b_bnck1_2)\n",
    "h_conv1_2 = conv2d(h_bnck1_2,W_conv1_2,b_conv1_2)\n",
    "h_bnck1_3 = conv2d(tf.concat((h_conv1,h_conv1_1,h_conv1_2),axis=3),W_bnck1_3,b_bnck1_3)\n",
    "h_conv1_3 = conv2d(h_bnck1_3,W_conv1_3,b_conv1_3)\n",
    "h_bnck1_4 = conv2d(tf.concat((h_conv1,h_conv1_1,h_conv1_2,h_conv1_3),axis=3),W_bnck1_4,b_bnck1_4)\n",
    "h_conv1_4 = conv2d(h_bnck1_4,W_conv1_4,b_conv1_4)\n",
    "h_bnck1_5 = conv2d(tf.concat((h_conv1,h_conv1_1,h_conv1_2,h_conv1_3,h_conv1_4),axis=3),W_bnck1_5,b_bnck1_5)\n",
    "h_conv1_5 = conv2d(h_bnck1_5,W_conv1_5,b_conv1_5)\n",
    "\n",
    "h_conv2 = max_pool_2x2(h_conv1_5)\n",
    "h_bnck2_1 = conv2d(h_conv2,W_bnck2_1,b_bnck2_1)\n",
    "h_conv2_1 = conv2d(h_bnck2_1,W_conv2_1,b_conv2_1)\n",
    "h_bnck2_2 = conv2d(tf.concat((h_conv2,h_conv2_1),axis=3),W_bnck2_2,b_bnck2_2)\n",
    "h_conv2_2 = conv2d(h_bnck2_2,W_conv2_2,b_conv2_2)\n",
    "h_bnck2_3 = conv2d(tf.concat((h_conv2,h_conv2_1,h_conv2_2),axis=3),W_bnck2_3,b_bnck2_3)\n",
    "h_conv2_3 = conv2d(h_bnck2_3,W_conv2_3,b_conv2_3)\n",
    "h_bnck2_4 = conv2d(tf.concat((h_conv2,h_conv2_1,h_conv2_2,h_conv2_3),axis=3),W_bnck2_4,b_bnck2_4)\n",
    "h_conv2_4 = conv2d(h_bnck2_4,W_conv2_4,b_conv2_4)\n",
    "h_bnck2_5 = conv2d(tf.concat((h_conv2,h_conv2_1,h_conv2_2,h_conv2_3,h_conv2_4),axis=3),W_bnck2_5,b_bnck2_5)\n",
    "h_conv2_5 = conv2d(h_bnck2_5,W_conv2_5,b_conv2_5)\n",
    "\n",
    "h_conv2_5_flat = tf.reshape(h_conv2_5, [-1, 14*14*k])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_conv2_5_flat, W_fc1) + b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# PREDICTION\n",
    "pred=tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% version = 1\n",
    "# variable\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "W_conv3 = weight_variable([3,3,64,])\n",
    "\n",
    "W_fc1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "# model\n",
    "h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# PREDICTION\n",
    "pred=tf.matmul(h_fc1_drop, W_fc2) + b_fc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% version = 2\n",
    "# variable\n",
    "W_conv1 = weight_variable([5,5,1,12])\n",
    "b_conv1 = bias_variable([12])\n",
    "W_conv2 = weight_variable([5,5,12,12])\n",
    "b_conv2 = bias_variable([12])\n",
    "W_conv3 = weight_variable([5,5,24,12])\n",
    "b_conv3 = bias_variable([12])\n",
    "W_conv4 = weight_variable([5,5,36,12])\n",
    "b_conv4 = bias_variable([12])\n",
    "W_conv5 = weight_variable([5,5,48,12])\n",
    "b_conv5 = bias_variable([12])\n",
    "\n",
    "W_fc1 = weight_variable([28 * 28 * 12, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "# model\n",
    "h_conv1 = tf.nn.relu(conv2d(x,W_conv1) + b_conv1)\n",
    "h_conv2 = tf.nn.relu(conv2d(h_conv1,W_conv2) + b_conv2)\n",
    "h_conv3 = tf.nn.relu(conv2d(tf.concat((h_conv1,h_conv2),axis=3),W_conv3) + b_conv3)\n",
    "h_conv4 = tf.nn.relu(conv2d(tf.concat((h_conv1,h_conv2,h_conv3),axis=3),W_conv4) + b_conv4)\n",
    "h_conv5 = tf.nn.relu(conv2d(tf.concat((h_conv1,h_conv2,h_conv3,h_conv4),axis=3),W_conv5) + b_conv5)\n",
    "\n",
    "h_conv5_flat = tf.reshape(h_conv5, [-1, 28*28*12])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_conv5_flat, W_fc1) + b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# PREDICTION\n",
    "pred=tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% version = 3\n",
    "# variable\n",
    "W_conv1_1 = weight_variable('conv1_1',[3,3,1,16])\n",
    "b_conv1_1 = bias_variable([16])\n",
    "W_conv1_2 = weight_variable('conv1_2',[3,3,16,16])\n",
    "b_conv1_2 = bias_variable([16])\n",
    "W_conv1_3 = weight_variable('conv1_3',[3,3,32,16])\n",
    "b_conv1_3 = bias_variable([16])\n",
    "W_conv1_4 = weight_variable('conv1_4',[3,3,48,16])\n",
    "b_conv1_4 = bias_variable([16])\n",
    "W_conv1_5 = weight_variable('conv1_5',[3,3,64,16])\n",
    "b_conv1_5 = bias_variable([16])\n",
    "\n",
    "#W_conv2_1 = weight_variable([3,3,1,16])\n",
    "#b_conv2_1 = bias_variable([16])\n",
    "W_conv2_2 = weight_variable('conv2_2',[3,3,16,16])\n",
    "b_conv2_2 = bias_variable([16])\n",
    "W_conv2_3 = weight_variable('conv2_3',[3,3,32,16])\n",
    "b_conv2_3 = bias_variable([16])\n",
    "W_conv2_4 = weight_variable('conv2_4',[3,3,48,16])\n",
    "b_conv2_4 = bias_variable([16])\n",
    "W_conv2_5 = weight_variable('conv2_5',[3,3,64,16])\n",
    "b_conv2_5 = bias_variable([16])\n",
    "\n",
    "W_fc1 = weight_variable('fc1',[14 * 14 * 16, 512])\n",
    "b_fc1 = bias_variable([512])\n",
    "W_fc2 = weight_variable('fc2',[512, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "# model\n",
    "h_conv1_1 = tf.nn.relu(conv2d(x,W_conv1_1) + b_conv1_1)\n",
    "h_conv1_2 = tf.nn.relu(conv2d(h_conv1_1,W_conv1_2) + b_conv1_2)\n",
    "h_conv1_3 = tf.nn.relu(conv2d(tf.concat((h_conv1_1,h_conv1_2),axis=3),W_conv1_3) + b_conv1_3)\n",
    "h_conv1_4 = tf.nn.relu(conv2d(tf.concat((h_conv1_1,h_conv1_2,h_conv1_3),axis=3),W_conv1_4) + b_conv1_4)\n",
    "h_conv1_5 = tf.nn.relu(conv2d(tf.concat((h_conv1_1,h_conv1_2,h_conv1_3,h_conv1_4),axis=3),W_conv1_5) + b_conv1_5)\n",
    "h_conv2_1 = max_pool_2x2(h_conv1_5)\n",
    "h_conv2_2 = tf.nn.relu(conv2d(h_conv2_1,W_conv2_2) + b_conv2_2)\n",
    "h_conv2_3 = tf.nn.relu(conv2d(tf.concat((h_conv2_1,h_conv2_2),axis=3),W_conv2_3) + b_conv2_3)\n",
    "h_conv2_4 = tf.nn.relu(conv2d(tf.concat((h_conv2_1,h_conv2_2,h_conv2_3),axis=3),W_conv2_4) + b_conv2_4)\n",
    "h_conv2_5 = tf.nn.relu(conv2d(tf.concat((h_conv2_1,h_conv2_2,h_conv2_3,h_conv2_4),axis=3),W_conv2_5) + b_conv2_5)\n",
    "#h_pool2 = max_pool_2x2(h_conv2_5)\n",
    "\n",
    "h_conv2_5_flat = tf.reshape(h_conv2_5, [-1, 14*14*16])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_conv2_5_flat, W_fc1) + b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# PREDICTION\n",
    "pred=tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#%% version = 4\n",
    "# variable\n",
    "W_conv1_1 = weight_variable('conv1_1',[3,3,1,16])\n",
    "b_conv1_1 = bias_variable([16])\n",
    "W_conv1_2 = weight_variable('conv1_2',[3,3,16,16])\n",
    "b_conv1_2 = bias_variable([16])\n",
    "W_conv1_3 = weight_variable('conv1_3',[3,3,32,16])\n",
    "b_conv1_3 = bias_variable([16])\n",
    "W_conv1_4 = weight_variable('conv1_4',[3,3,48,16])\n",
    "b_conv1_4 = bias_variable([16])\n",
    "W_conv1_5 = weight_variable('conv1_5',[3,3,64,16])\n",
    "b_conv1_5 = bias_variable([16])\n",
    "\n",
    "#W_conv2_1 = weight_variable([3,3,1,16])\n",
    "#b_conv2_1 = bias_variable([16])\n",
    "W_conv2_2 = weight_variable('conv2_2',[3,3,16,16])\n",
    "b_conv2_2 = bias_variable([16])\n",
    "W_conv2_3 = weight_variable('conv2_3',[3,3,32,16])\n",
    "b_conv2_3 = bias_variable([16])\n",
    "W_conv2_4 = weight_variable('conv2_4',[3,3,48,16])\n",
    "b_conv2_4 = bias_variable([16])\n",
    "W_conv2_5 = weight_variable('conv2_5',[3,3,64,16])\n",
    "b_conv2_5 = bias_variable([16])\n",
    "\n",
    "#W_conv3_1 = weight_variable([3,3,1,16])\n",
    "#b_conv3_1 = bias_variable([16])\n",
    "W_conv3_2 = weight_variable('conv3_2',[3,3,16,16])\n",
    "b_conv3_2 = bias_variable([16])\n",
    "W_conv3_3 = weight_variable('conv3_3',[3,3,32,16])\n",
    "b_conv3_3 = bias_variable([16])\n",
    "W_conv3_4 = weight_variable('conv3_4',[3,3,48,16])\n",
    "b_conv3_4 = bias_variable([16])\n",
    "W_conv3_5 = weight_variable('conv3_5',[3,3,64,16])\n",
    "b_conv3_5 = bias_variable([16])\n",
    "\n",
    "W_fc1 = weight_variable('fc1',[7 * 7 * 16, 512])\n",
    "b_fc1 = bias_variable([512])\n",
    "W_fc2 = weight_variable('fc2',[512, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "# model\n",
    "h_conv1_1 = conv2d(x,W_conv1_1,b_conv1_1)\n",
    "h_conv1_2 = conv2d(h_conv1_1,W_conv1_2,b_conv1_2)\n",
    "h_conv1_3 = conv2d(tf.concat((h_conv1_1,h_conv1_2),axis=3),W_conv1_3,b_conv1_3)\n",
    "h_conv1_4 = conv2d(tf.concat((h_conv1_1,h_conv1_2,h_conv1_3),axis=3),W_conv1_4,b_conv1_4)\n",
    "h_conv1_5 = conv2d(tf.concat((h_conv1_1,h_conv1_2,h_conv1_3,h_conv1_4),axis=3),W_conv1_5,b_conv1_5)\n",
    "h_conv2_1 = max_pool_2x2(h_conv1_5)\n",
    "h_conv2_2 = conv2d(h_conv2_1,W_conv2_2,b_conv2_2)\n",
    "h_conv2_3 = conv2d(tf.concat((h_conv2_1,h_conv2_2),axis=3),W_conv2_3,b_conv2_3)\n",
    "h_conv2_4 = conv2d(tf.concat((h_conv2_1,h_conv2_2,h_conv2_3),axis=3),W_conv2_4,b_conv2_4)\n",
    "h_conv2_5 = conv2d(tf.concat((h_conv2_1,h_conv2_2,h_conv2_3,h_conv2_4),axis=3),W_conv2_5,b_conv2_5)\n",
    "h_conv3_1 = max_pool_2x2(h_conv2_5)\n",
    "h_conv3_2 = conv2d(h_conv3_1,W_conv3_2,b_conv3_2)\n",
    "h_conv3_3 = conv2d(tf.concat((h_conv3_1,h_conv3_2),axis=3),W_conv3_3,b_conv3_3)\n",
    "h_conv3_4 = conv2d(tf.concat((h_conv3_1,h_conv3_2,h_conv3_3),axis=3),W_conv3_4,b_conv3_4)\n",
    "h_conv3_5 = conv2d(tf.concat((h_conv3_1,h_conv3_2,h_conv3_3,h_conv3_4),axis=3),W_conv3_5,b_conv3_5)\n",
    "\n",
    "h_conv3_5_flat = tf.reshape(h_conv3_5, [-1, 7*7*16])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_conv3_5_flat, W_fc1) + b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# PREDICTION\n",
    "pred=tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv2d() takes exactly 3 arguments (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-85da1b9219c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mh_conv1_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW_conv1_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mh_conv1_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_conv1_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW_conv1_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mh_conv1_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_conv1_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mh_conv1_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mW_conv1_3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: conv2d() takes exactly 3 arguments (2 given)"
     ]
    }
   ],
   "source": [
    "#%% version = 5\n",
    "# variable\n",
    "W_conv1_1 = weight_variable('conv1_1',[3,3,1,16])\n",
    "b_conv1_1 = bias_variable([16])\n",
    "W_conv1_2 = weight_variable('conv1_2',[3,3,16,16])\n",
    "b_conv1_2 = bias_variable([16])\n",
    "W_conv1_3 = weight_variable('conv1_3',[3,3,32,16])\n",
    "b_conv1_3 = bias_variable([16])\n",
    "W_conv1_4 = weight_variable('conv1_4',[3,3,48,16])\n",
    "b_conv1_4 = bias_variable([16])\n",
    "W_conv1_5 = weight_variable('conv1_5',[3,3,64,16])\n",
    "b_conv1_5 = bias_variable([16])\n",
    "\n",
    "#W_conv2_1 = weight_variable([3,3,1,16])\n",
    "#b_conv2_1 = bias_variable([16])\n",
    "W_conv2_2 = weight_variable('conv2_2',[3,3,16,16])\n",
    "b_conv2_2 = bias_variable([16])\n",
    "W_conv2_3 = weight_variable('conv2_3',[3,3,32,16])\n",
    "b_conv2_3 = bias_variable([16])\n",
    "W_conv2_4 = weight_variable('conv2_4',[3,3,48,16])\n",
    "b_conv2_4 = bias_variable([16])\n",
    "W_conv2_5 = weight_variable('conv2_5',[3,3,64,16])\n",
    "b_conv2_5 = bias_variable([16])\n",
    "\n",
    "#W_conv3_1 = weight_variable([3,3,1,16])\n",
    "#b_conv3_1 = bias_variable([16])\n",
    "W_conv3_2 = weight_variable('conv3_2',[3,3,16,16])\n",
    "b_conv3_2 = bias_variable([16])\n",
    "W_conv3_3 = weight_variable('conv3_3',[3,3,32,16])\n",
    "b_conv3_3 = bias_variable([16])\n",
    "W_conv3_4 = weight_variable('conv3_4',[3,3,48,16])\n",
    "b_conv3_4 = bias_variable([16])\n",
    "W_conv3_5 = weight_variable('conv3_5',[3,3,64,16])\n",
    "b_conv3_5 = bias_variable([16])\n",
    "\n",
    "W_fc1 = weight_variable('fc1',[7 * 7 * 16, 512])\n",
    "b_fc1 = bias_variable([512])\n",
    "W_fc2 = weight_variable('fc2',[512, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "# model\n",
    "h_conv1_1 = conv2d(x,W_conv1_1)\n",
    "h_conv1_2 = conv2d(h_conv1_1,W_conv1_2)\n",
    "h_conv1_3 = conv2d(tf.concat((h_conv1_1,h_conv1_2),axis=3),W_conv1_3)\n",
    "h_conv1_4 = conv2d(tf.concat((h_conv1_1,h_conv1_2,h_conv1_3),axis=3),W_conv1_4)\n",
    "h_conv1_5 = conv2d(tf.concat((h_conv1_1,h_conv1_2,h_conv1_3,h_conv1_4),axis=3),W_conv1_5)\n",
    "h_conv2_1 = max_pool_2x2(h_conv1_5)\n",
    "h_conv2_2 = conv2d(h_conv2_1,W_conv2_2)\n",
    "h_conv2_3 = conv2d(tf.concat((h_conv2_1,h_conv2_2),axis=3),W_conv2_3)\n",
    "h_conv2_4 = conv2d(tf.concat((h_conv2_1,h_conv2_2,h_conv2_3),axis=3),W_conv2_4)\n",
    "h_conv2_5 = conv2d(tf.concat((h_conv2_1,h_conv2_2,h_conv2_3,h_conv2_4),axis=3),W_conv2_5)\n",
    "h_conv3_1 = max_pool_2x2(h_conv2_5)\n",
    "h_conv3_2 = conv2d(h_conv3_1,W_conv3_2)\n",
    "h_conv3_3 = conv2d(tf.concat((h_conv3_1,h_conv3_2),axis=3),W_conv3_3)\n",
    "h_conv3_4 = conv2d(tf.concat((h_conv3_1,h_conv3_2,h_conv3_3),axis=3),W_conv3_4)\n",
    "h_conv3_5 = conv2d(tf.concat((h_conv3_1,h_conv3_2,h_conv3_3,h_conv3_4),axis=3),W_conv3_5)\n",
    "\n",
    "h_conv3_5_flat = tf.reshape(h_conv3_5, [-1, 7*7*16])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_conv3_5_flat, W_fc1) + b_fc1)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)\n",
    "\n",
    "# PREDICTION\n",
    "pred=tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUNCTIONS READY\n"
     ]
    }
   ],
   "source": [
    "# LOSS AND OPTIMIZER\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred))\n",
    "optm = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost) \n",
    "corr = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))    \n",
    "accr = tf.reduce_mean(tf.cast(corr, \"float\"))\n",
    "\n",
    "# INITIALIZER\n",
    "init = tf.global_variables_initializer()\n",
    "print (\"FUNCTIONS READY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/050, Cost: 0.425268\n",
      " TRAIN ACCURACY: 0.842\n",
      " TEST ACCURACY: 0.870\n",
      "Epoch: 002/050, Cost: 0.281682\n",
      " TRAIN ACCURACY: 0.926\n",
      " TEST ACCURACY: 0.885\n",
      "Epoch: 003/050, Cost: 0.233321\n",
      " TRAIN ACCURACY: 0.937\n",
      " TEST ACCURACY: 0.889\n",
      "Epoch: 004/050, Cost: 0.202222\n",
      " TRAIN ACCURACY: 0.958\n",
      " TEST ACCURACY: 0.899\n",
      "************************\n",
      "time for 5 epoch : 146.375482082\n",
      "************************\n",
      "Epoch: 005/050, Cost: 0.176872\n",
      " TRAIN ACCURACY: 0.916\n",
      " TEST ACCURACY: 0.898\n",
      "Epoch: 006/050, Cost: 0.159690\n",
      " TRAIN ACCURACY: 0.989\n",
      " TEST ACCURACY: 0.897\n",
      "Epoch: 007/050, Cost: 0.139010\n",
      " TRAIN ACCURACY: 0.916\n",
      " TEST ACCURACY: 0.896\n",
      "Epoch: 008/050, Cost: 0.125813\n",
      " TRAIN ACCURACY: 0.926\n",
      " TEST ACCURACY: 0.903\n",
      "Epoch: 009/050, Cost: 0.111769\n",
      " TRAIN ACCURACY: 0.958\n",
      " TEST ACCURACY: 0.903\n",
      "Epoch: 010/050, Cost: 0.099255\n",
      " TRAIN ACCURACY: 0.968\n",
      " TEST ACCURACY: 0.903\n",
      "Epoch: 011/050, Cost: 0.092444\n",
      " TRAIN ACCURACY: 0.968\n",
      " TEST ACCURACY: 0.896\n",
      "Epoch: 012/050, Cost: 0.084446\n",
      " TRAIN ACCURACY: 0.947\n",
      " TEST ACCURACY: 0.902\n",
      "Epoch: 013/050, Cost: 0.077028\n",
      " TRAIN ACCURACY: 0.979\n",
      " TEST ACCURACY: 0.906\n",
      "Epoch: 014/050, Cost: 0.068380\n",
      " TRAIN ACCURACY: 0.979\n",
      " TEST ACCURACY: 0.894\n",
      "Epoch: 015/050, Cost: 0.065387\n",
      " TRAIN ACCURACY: 0.947\n",
      " TEST ACCURACY: 0.895\n",
      "Epoch: 016/050, Cost: 0.063958\n",
      " TRAIN ACCURACY: 0.979\n",
      " TEST ACCURACY: 0.904\n",
      "Epoch: 017/050, Cost: 0.057863\n",
      " TRAIN ACCURACY: 0.968\n",
      " TEST ACCURACY: 0.901\n",
      "Epoch: 018/050, Cost: 0.058111\n",
      " TRAIN ACCURACY: 0.958\n",
      " TEST ACCURACY: 0.902\n",
      "Epoch: 019/050, Cost: 0.056181\n",
      " TRAIN ACCURACY: 0.979\n",
      " TEST ACCURACY: 0.902\n",
      "Epoch: 020/050, Cost: 0.053429\n",
      " TRAIN ACCURACY: 0.989\n",
      " TEST ACCURACY: 0.897\n",
      "Epoch: 021/050, Cost: 0.047628\n",
      " TRAIN ACCURACY: 1.000\n",
      " TEST ACCURACY: 0.901\n",
      "Epoch: 022/050, Cost: 0.048149\n",
      " TRAIN ACCURACY: 0.979\n",
      " TEST ACCURACY: 0.898\n",
      "Epoch: 023/050, Cost: 0.046833\n",
      " TRAIN ACCURACY: 0.979\n",
      " TEST ACCURACY: 0.899\n",
      "Epoch: 024/050, Cost: 0.042551\n",
      " TRAIN ACCURACY: 0.989\n",
      " TEST ACCURACY: 0.901\n",
      "Epoch: 025/050, Cost: 0.042297\n",
      " TRAIN ACCURACY: 1.000\n",
      " TEST ACCURACY: 0.905\n",
      "Epoch: 026/050, Cost: 0.042556\n",
      " TRAIN ACCURACY: 0.968\n",
      " TEST ACCURACY: 0.898\n",
      "Epoch: 027/050, Cost: 0.038236\n",
      " TRAIN ACCURACY: 0.979\n",
      " TEST ACCURACY: 0.898\n",
      "Epoch: 028/050, Cost: 0.037124\n",
      " TRAIN ACCURACY: 0.979\n",
      " TEST ACCURACY: 0.896\n",
      "Epoch: 029/050, Cost: 0.038750\n",
      " TRAIN ACCURACY: 0.989\n",
      " TEST ACCURACY: 0.902\n",
      "Epoch: 030/050, Cost: 0.038465\n",
      " TRAIN ACCURACY: 0.989\n",
      " TEST ACCURACY: 0.899\n",
      "Epoch: 031/050, Cost: 0.035004\n",
      " TRAIN ACCURACY: 0.989\n",
      " TEST ACCURACY: 0.901\n",
      "Epoch: 032/050, Cost: 0.037479\n",
      " TRAIN ACCURACY: 0.989\n",
      " TEST ACCURACY: 0.905\n",
      "Epoch: 033/050, Cost: 0.039629\n",
      " TRAIN ACCURACY: 0.989\n",
      " TEST ACCURACY: 0.901\n",
      "Epoch: 034/050, Cost: 0.034192\n",
      " TRAIN ACCURACY: 0.968\n",
      " TEST ACCURACY: 0.898\n",
      "Epoch: 035/050, Cost: 0.032409\n",
      " TRAIN ACCURACY: 1.000\n",
      " TEST ACCURACY: 0.905\n",
      "Epoch: 036/050, Cost: 0.036627\n",
      " TRAIN ACCURACY: 0.958\n",
      " TEST ACCURACY: 0.902\n",
      "Epoch: 037/050, Cost: 0.035157\n",
      " TRAIN ACCURACY: 0.989\n",
      " TEST ACCURACY: 0.901\n",
      "Epoch: 038/050, Cost: 0.034665\n",
      " TRAIN ACCURACY: 0.979\n",
      " TEST ACCURACY: 0.904\n",
      "Epoch: 039/050, Cost: 0.027440\n",
      " TRAIN ACCURACY: 0.989\n",
      " TEST ACCURACY: 0.903\n",
      "Epoch: 040/050, Cost: 0.032418\n",
      " TRAIN ACCURACY: 0.989\n",
      " TEST ACCURACY: 0.902\n",
      "Epoch: 041/050, Cost: 0.029034\n",
      " TRAIN ACCURACY: 1.000\n",
      " TEST ACCURACY: 0.905\n",
      "Epoch: 042/050, Cost: 0.032088\n",
      " TRAIN ACCURACY: 0.989\n",
      " TEST ACCURACY: 0.903\n",
      "Epoch: 043/050, Cost: 0.028515\n",
      " TRAIN ACCURACY: 0.989\n",
      " TEST ACCURACY: 0.907\n",
      "Epoch: 044/050, Cost: 0.032007\n",
      " TRAIN ACCURACY: 0.989\n",
      " TEST ACCURACY: 0.897\n",
      "Epoch: 045/050, Cost: 0.030309\n",
      " TRAIN ACCURACY: 0.989\n",
      " TEST ACCURACY: 0.903\n",
      "Epoch: 046/050, Cost: 0.030058\n",
      " TRAIN ACCURACY: 1.000\n",
      " TEST ACCURACY: 0.904\n",
      "Epoch: 047/050, Cost: 0.028432\n",
      " TRAIN ACCURACY: 1.000\n",
      " TEST ACCURACY: 0.902\n",
      "Epoch: 048/050, Cost: 0.026135\n",
      " TRAIN ACCURACY: 0.979\n",
      " TEST ACCURACY: 0.901\n",
      "Epoch: 049/050, Cost: 0.028183\n",
      " TRAIN ACCURACY: 0.989\n",
      " TEST ACCURACY: 0.900\n",
      "Epoch: 050/050, Cost: 0.029510\n",
      " TRAIN ACCURACY: 0.968\n",
      " TEST ACCURACY: 0.905\n",
      "*************************\n",
      "OPTIMIZATION FINISHED\n",
      "*************************\n",
      " TEST ACCURACY: 0.907\n",
      "time: 0.240605831146\n"
     ]
    }
   ],
   "source": [
    "# PARAMETERS\n",
    "training_epochs = 50\n",
    "batch_size      = 128\n",
    "disp_each       = 1\n",
    "\n",
    "# LAUNCH THE GRAPH\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)\n",
    "# OPTIMIZE\n",
    "time_for_5_epoch = time.time()\n",
    "for epoch in range(training_epochs):\n",
    "    total_batch = int(ntrain/batch_size)+1\n",
    "    randpermlist = np.random.permutation(ntrain)\n",
    "    sun_cost = 0.\n",
    "    for i in range(total_batch):\n",
    "        randidx  = randpermlist[i*batch_size:min((i+1)*batch_size, ntrain-1)]\n",
    "        batch_xs = X_train[randidx, :]\n",
    "        batch_ys = y_train[randidx, :]                \n",
    "        feeds = {x: batch_xs, y: batch_ys, keep_prob: 0.5}\n",
    "        sess.run(optm, feed_dict=feeds)\n",
    "        sun_cost += sess.run(cost, feed_dict=feeds)\n",
    "    avg_cost = sun_cost / total_batch\n",
    "   \n",
    "    # 5 epoch time\n",
    "    if epoch == 4:\n",
    "        print(\"************************\")\n",
    "        print(\"time for 5 epoch : {}\".format(time.time() - time_for_5_epoch))\n",
    "        print(\"************************\")\n",
    "        \n",
    "    # DISPLAY\n",
    "    if (epoch+1) % disp_each == 0 or epoch == training_epochs-1:\n",
    "        print (\"Epoch: %03d/%03d, Cost: %f\" % (epoch+1, training_epochs, avg_cost))\n",
    "        feeds = {x: batch_xs, y: batch_ys, keep_prob: 0.5}\n",
    "        train_acc = sess.run(accr, feed_dict=feeds)\n",
    "        print (\" TRAIN ACCURACY: %.3f\" % (train_acc))\n",
    "        feeds = {x: X_test, y: y_test, keep_prob: 0.5}\n",
    "        test_acc = sess.run(accr, feed_dict=feeds)\n",
    "        print (\" TEST ACCURACY: %.3f\" % (test_acc))\n",
    "\n",
    "    \n",
    "print (\"*************************\")\n",
    "print (\"OPTIMIZATION FINISHED\")\n",
    "print (\"*************************\")\n",
    "\n",
    "current = time.time()\n",
    "feeds = {x: X_test, y: y_test, keep_prob: 0.5}\n",
    "test_acc = sess.run(accr, feed_dict=feeds)\n",
    "print (\" TEST ACCURACY: %.3f\" % (test_acc))\n",
    "print(\"time: {}\".format(time.time() - current))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-469a83017b19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
